{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547dfe85",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Flow Matching\n",
    "\n",
    "Notebook to implement a number of different simulation-free methods for learning flow models.\n",
    "\n",
    "In this notebook we implement 5 models that can map from a source distribution $q_0$ to a target distribution $q_1$:\n",
    "* Conditional Flow Matching (CFM)\n",
    "    * \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" [(Liu et al. 2023)](https://openreview.net/forum?id=XVjTT1nw5z)\n",
    "    * \"Stochastic Interpolants\" [(Albergo et al. 2023)](https://openreview.net/forum?id=li7qeBbCR1t) with a non-variance preserving interpolant.\n",
    "    * \"Flow Matching\" [(Lipman et al. 2023)](https://openreview.net/forum?id=PqvMRDCJT9t) but conditions on both source and target.\n",
    "* Optimal Transport CFM (OT-CFM), which directly optimizes for dynamic optimal transport (WIP)\n",
    "* Variational Flow Matching (VFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb51734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Engine import *\n",
    "from pathlib import Path\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import generate_moons\n",
    "\n",
    "savedir = \"Results/Moons\"\n",
    "Path(savedir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b241a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditional Flow Matching\n",
    "\n",
    "First we implement the basic conditional flow matching. As in the paper, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= (x_0, x_1) \\\\\n",
    "q(z) &= q(x_0)q(x_1) \\\\\n",
    "p_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\n",
    "u_t(x | z) &= x_1 - x_0\n",
    "\\end{align}\n",
    "$$\n",
    "When $\\sigma = 0$ this is equivalent to zero-steps of rectified flow. We find that small $\\sigma$ helps to regularize the problem ymmv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176eb7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "model = MLP(dim=dim, time_varying=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "FM = CFM(sigma=sigma)\n",
    "\n",
    "start = time.time()\n",
    "for k in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (k + 1) % 5000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "\n",
    "        node = NeuralODE(torch_wrapper(model), solver=\"euler\")\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(1024),\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )\n",
    "            plot_trajectories(traj=traj.cpu().numpy(), output=f\"{savedir}/CFM_{k+1}.png\")\n",
    "        \n",
    "        evaluate(traj[-1].cpu(), sample_moons(1024))\n",
    "            \n",
    "torch.save(model, f\"{savedir}/CFM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8057b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimal Transport Conditional Flow Matching\n",
    "\n",
    "Next we implement optimal transport conditional flow matching. As in the paper, here we have\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= (x_0, x_1) \\\\\n",
    "q(z) &= \\pi(x_0, x_1) \\\\\n",
    "p_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\n",
    "u_t(x | z) &= x_1 - x_0\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\pi$ is the joint of an exact optimal transport matrix. We first sample random $x_0, x_1$, then resample according to the optimal transport matrix as computed with the python optimal transport package. We use the 2-Wasserstein distance with an $L^2$ ground distance for equivalence with dynamic optimal transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed74817",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "model = MLP(dim=dim, time_varying=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "FM = OT_CFM(sigma=sigma)\n",
    "\n",
    "start = time.time()\n",
    "for k in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (k + 1) % 5000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(torch_wrapper(model), solver=\"euler\")\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(1024),\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )\n",
    "            plot_trajectories(traj=traj.cpu().numpy(), output=f\"{savedir}/OT-CFM_{k+1}.png\")\n",
    "        \n",
    "        evaluate(traj[-1].cpu(), sample_moons(1024))\n",
    "            \n",
    "torch.save(model, f\"{savedir}/OT-CFM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef9f97",
   "metadata": {},
   "source": [
    "### Lipman's Optimal Transport Conditional Flow Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256 * 4\n",
    "model = MLP(dim=dim, time_varying=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "FM = LOT_CFM(sigma=sigma)\n",
    "\n",
    "start = time.time()\n",
    "for k in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "    psi_t = FM.psi_t(x1, x0, t)\n",
    "    vt = model(torch.cat([psi_t, t[:, None]], dim=-1))\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (k + 1) % 5000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(torch_wrapper(model), solver=\"euler\")\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(1024),\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )\n",
    "            plot_trajectories(traj=traj.cpu().numpy(), output=f\"{savedir}/LOT-CFM_{k+1}.png\")\n",
    "        \n",
    "        evaluate(traj[-1].cpu(), sample_moons(1024))\n",
    "            \n",
    "torch.save(model, f\"{savedir}/LOT-CFM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc23dc",
   "metadata": {},
   "source": [
    "### Variational Flow Matching\n",
    "\n",
    "Next we implement variational flow matching, which corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1b2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000: loss 39.635 time 36.31\n",
      "Fréchet Distance: 0.7450. Hausdorff Distance: 2.4823. Energy Distance: 0.1982.\n",
      "\n",
      "10000: loss 17.081 time 38.51\n",
      "Fréchet Distance: 0.0527. Hausdorff Distance: 2.5969. Energy Distance: 0.1030.\n",
      "\n",
      "15000: loss 16.758 time 35.85\n",
      "Fréchet Distance: 0.1534. Hausdorff Distance: 3.5174. Energy Distance: 0.0945.\n",
      "\n",
      "20000: loss 19.866 time 40.05\n",
      "Fréchet Distance: 0.0544. Hausdorff Distance: 2.4354. Energy Distance: 0.1098.\n",
      "\n",
      "25000: loss 98.194 time 43.29\n",
      "Fréchet Distance: 0.0101. Hausdorff Distance: 2.3636. Energy Distance: 0.0781.\n",
      "\n",
      "30000: loss 612.881 time 41.46\n",
      "Fréchet Distance: 0.4497. Hausdorff Distance: 5.5201. Energy Distance: 0.2798.\n",
      "\n",
      "35000: loss 33.825 time 41.73\n",
      "Fréchet Distance: 1.3140. Hausdorff Distance: 2.2207. Energy Distance: 0.3483.\n",
      "\n",
      "40000: loss 37.171 time 43.05\n",
      "Fréchet Distance: 0.1697. Hausdorff Distance: 2.5171. Energy Distance: 0.1142.\n",
      "\n",
      "45000: loss 115.347 time 43.80\n",
      "Fréchet Distance: 0.1534. Hausdorff Distance: 2.2687. Energy Distance: 0.0850.\n",
      "\n",
      "50000: loss 14.622 time 44.36\n",
      "Fréchet Distance: 0.0458. Hausdorff Distance: 2.5908. Energy Distance: 0.0849.\n",
      "\n",
      "55000: loss 104.389 time 41.92\n",
      "Fréchet Distance: 1.4783. Hausdorff Distance: 4.4836. Energy Distance: 0.2990.\n",
      "\n",
      "60000: loss 68.750 time 41.97\n",
      "Fréchet Distance: 0.1022. Hausdorff Distance: 1.5013. Energy Distance: 0.1113.\n",
      "\n",
      "65000: loss 71089.688 time 47.27\n",
      "Fréchet Distance: 0.5614. Hausdorff Distance: 1.6333. Energy Distance: 0.1335.\n",
      "\n",
      "70000: loss 45.142 time 40.95\n",
      "Fréchet Distance: 0.0664. Hausdorff Distance: 1.7141. Energy Distance: 0.1087.\n",
      "\n",
      "75000: loss 27.517 time 43.90\n",
      "Fréchet Distance: 0.0414. Hausdorff Distance: 4.3621. Energy Distance: 0.0851.\n",
      "\n",
      "80000: loss 130.342 time 40.49\n",
      "Fréchet Distance: 3.6041. Hausdorff Distance: 2.5643. Energy Distance: 0.3608.\n",
      "\n",
      "85000: loss 58.853 time 38.72\n",
      "Fréchet Distance: 0.2214. Hausdorff Distance: 1.2427. Energy Distance: 0.1099.\n",
      "\n",
      "90000: loss 15.346 time 36.52\n",
      "Fréchet Distance: 0.0540. Hausdorff Distance: 1.8777. Energy Distance: 0.0953.\n",
      "\n",
      "95000: loss 29.979 time 36.94\n",
      "Fréchet Distance: 0.7816. Hausdorff Distance: 1.4724. Energy Distance: 0.2464.\n",
      "\n",
      "100000: loss 24.532 time 44.21\n",
      "Fréchet Distance: 0.5118. Hausdorff Distance: 1.3744. Energy Distance: 0.2260.\n",
      "\n",
      "105000: loss 55.590 time 41.98\n",
      "Fréchet Distance: 0.0213. Hausdorff Distance: 1.3943. Energy Distance: 0.1034.\n",
      "\n",
      "110000: loss 15.065 time 51.42\n",
      "Fréchet Distance: 0.0459. Hausdorff Distance: 1.9780. Energy Distance: 0.0985.\n",
      "\n",
      "115000: loss 56.469 time 47.67\n",
      "Fréchet Distance: 0.0759. Hausdorff Distance: 1.5561. Energy Distance: 0.0792.\n",
      "\n",
      "120000: loss 38.586 time 43.73\n",
      "Fréchet Distance: 0.8576. Hausdorff Distance: 4.4341. Energy Distance: 0.2923.\n",
      "\n",
      "125000: loss 16.610 time 49.22\n",
      "Fréchet Distance: 0.1094. Hausdorff Distance: 1.7414. Energy Distance: 0.0851.\n",
      "\n",
      "130000: loss 52.264 time 52.85\n",
      "Fréchet Distance: 0.0317. Hausdorff Distance: 1.3857. Energy Distance: 0.1222.\n",
      "\n",
      "135000: loss 31.565 time 49.78\n",
      "Fréchet Distance: 0.3808. Hausdorff Distance: 1.9761. Energy Distance: 0.1206.\n",
      "\n",
      "140000: loss 122.289 time 38.30\n",
      "Fréchet Distance: 0.0868. Hausdorff Distance: 1.5719. Energy Distance: 0.0970.\n",
      "\n",
      "145000: loss 63.051 time 41.25\n",
      "Fréchet Distance: 0.1506. Hausdorff Distance: 1.9915. Energy Distance: 0.1057.\n",
      "\n",
      "150000: loss 140.588 time 38.92\n",
      "Fréchet Distance: 0.1159. Hausdorff Distance: 9.7928. Energy Distance: 0.1362.\n",
      "\n",
      "155000: loss 287.820 time 39.60\n",
      "Fréchet Distance: 0.1027. Hausdorff Distance: 3.5445. Energy Distance: 0.0983.\n",
      "\n",
      "160000: loss 23.134 time 37.90\n",
      "Fréchet Distance: 0.0315. Hausdorff Distance: 1.3052. Energy Distance: 0.0675.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Vault/XVFM/Engine/models.py:21\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/XVFM/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sigma = 0.1\n",
    "dim = 2\n",
    "batch_size = 256\n",
    "model = MLP(dim=dim, time_varying=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-4, eps=1e-12)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)\n",
    "FM = VFM(sigma=sigma)\n",
    "\n",
    "start = time.time()\n",
    "for k in range(200000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = sample_8gaussians(batch_size)\n",
    "    x1 = sample_moons(batch_size)\n",
    "\n",
    "    t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "    mean = model(torch.cat([xt, t[:, None]], dim=-1))\n",
    "\n",
    "    # var = torch.tensor(sigma)**2\n",
    "\n",
    "    loss = torch.mean((mean - ut) ** 2)\n",
    "\n",
    "    # loss = -0.5 * torch.sum(-torch.log(2 * torch.pi * var) - ((x1 - mean)**2 / var))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if (k + 1) % 5000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n",
    "        start = end\n",
    "        node = NeuralODE(torch_wrapper(model), solver=\"euler\")\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                sample_8gaussians(1024),\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )\n",
    "            plot_trajectories(traj=traj.cpu().numpy(), output=f\"{savedir}/VFM_{k+1}.png\")\n",
    "        \n",
    "        evaluate(traj[-1].cpu(), sample_moons(1024))\n",
    "            \n",
    "torch.save(model, f\"{savedir}/VFM.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XVFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
